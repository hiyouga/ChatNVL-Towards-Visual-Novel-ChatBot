# 6 月 16 日实验日志

作者：hiyouga

数据集：koikake_yui, koikake_yui_desc

实验环境：V100 * 1

---

时隔上次的实验已经有一个多月了，近期发现指令调优模型的微调效果实在是过于不理想，因此打算放弃原先的 ChatGLM-6B 模型，使用预训练模型而非指令调优模型进行实验。

在这次的实验中，除了更换基础模型为基于 LLaMA-13B 的 ziya 预训练模型以外，还实现了多轮对话的同时训练（类似 FastChat），同时使用滑动窗口算法重新构建了多轮对话数据集。

首先进行了一个简单的测试，使用人物对话数据集在模型上微调了 10 个 epoch。

```bash
CUDA_VISIBLE_DEVICES=0 python ~/llama_tuning/src/train_sft.py \
    --model_name_or_path /home/incoming/zhengyw/llama/ziya-13b-ptv1 \
    --do_train \
    --dataset koikake_yui,koikake_yui_desc \
    --dataset_dir ../data \
    --finetuning_type lora \
    --lora_rank 8 \
    --lora_target q_proj,k_proj,v_proj,o_proj \
    --output_dir out/yui_230616_lora_sft_test \
    --overwrite_cache \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --max_source_length 512 \
    --max_target_length 512 \
    --prompt_template ziya \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 100 \
    --learning_rate 1e-5 \
    --max_grad_norm 0.5 \
    --warmup_steps 100 \
    --num_train_epochs 10.0 \
    --source_prefix 现在你需要扮演结衣和我对话，你是御影浜学校的少女，喜欢学长、绘本、安静的地方、花和小动物。你的回复需要根据角色性格特点，以结衣的语言风格和我对话。 \
    --plot_loss \
    --fp16
```

因为数据集中样本很少，只有 300 多条，所以训练步数只有 120 步左右。训练损失曲线如下图所示。

![1.jpg](media/230616_1.png)

启动一个 Web 服务端用来调试模型。

```bash
python src/web_demo.py \
    --model_name_or_path /home/incoming/zhengyw/llama/ziya-13b-ptv1/ \
    --checkpoint_dir ~/chatnvl/script/out/yui_230616_lora_sft_test/ \
    --prompt_template ziya \
    --source_prefix 现在你需要扮演结衣和我对话，你是御影浜学校的少女，喜欢学长、绘本、安静的地方、花和小动物。你的回复需要根据角色性格特点，以结衣的语言风格和我对话。
```

使用微调后的模型进行了一些简单的多轮对话，效果简直不要比之前好太多~

接下来的实验应该很有希望了！

![2.jpg](media/230616_2.jpg)

![3.jpg](media/230616_3.jpg)

![4.jpg](media/230616_4.jpg)

![5.jpg](media/230616_5.jpg)