# 4 月 24 日实验日志

作者：hiyouga

数据集：koikake_all_train

实验环境：A100 * 8

---

接着昨天的实验，我们在下面表格中详细列出不同指标下的评估结果，表格每行 id 与[前一个日志](230423.md)中的 id 一一对应。

下面是一些主要结论：

- Freeze 方法微调前 3 层会导致 alpaca 数据集的指标显著下降，说明模型知识被严重破坏。
- Freeze 方法微调 MLP 和 QKV 的结果相差不大。但似乎微调 MLP 对知识的影响大于微调 QKV。
- P-Tuning 方法时 alpaca 数据集指标几乎没有下降，说明对模型的知识破坏最小。但是实际测试效果看，P-Tuning 方法没有学习到角色的对话风格，之后可以尝试增大学习率。
- LoRA 微调中是否使用辅助训练集对 alpaca 数据集的指标关系很大。
- 从指标中没有看出 LoRA 微调 MLP 和 QKV 的显著区别。但是测试时微调 MLP 的回答并不理想。

| id | koikake | - | - | - | alpaca | - | - | - |
| -- | ------- | - | - | - | ------ | - | - | - |
| - | BLEU-4 | ROUGE-1 | ROUGE-2 | ROUGE-l | BLEU-4 | ROUGE-1 | ROUGE-2 | ROUGE-l |
| 0 | 1.43 | 11.93 | 0.74 | 8.78 | 15.65 | **34.41** | 15.67 | 26.20 |
| 1 | 2.49 | 12.13 | **1.28** | 10.23 | 9.56 | 26.83 | 9.58 | 19.07 |
| **2** | 2.42 | **14.29** | **1.12** | **11.55** | 8.77 | 29.03 | 10.63 | 21.44 |
| 3 | 2.15 | 12.48 | 0.84 | 10.33 | 2.42 | 14.39 | 3.74 | 10.88 |
| 4 | 2.16 | 11.85 | 0.81 | 9.48 | 5.65 | 24.35 | 8.20 | 18.05 |
| 5 | 1.86 | 12.06 | 0.80 | 9.34 | 8.60 | 25.48 | 8.78 | 18.80 |
| **6** | 2.17 | **14.26** | 0.59 | **11.19** | 9.57 | **30.16** | 11.79 | 22.62 |
| 7 | 2.18 | 12.57 | **1.00** | 9.62 | 2.05 | 10.03 | 2.62 | 8.31 |
| 8 | 2.19 | 12.67 | 0.70 | 10.62 | 1.96 | 11.78 | 3.45 | 8.89 |
| 9 | 1.37 | 11.89 | 0.71 | 8.93 | 16.37 | **35.59** | 15.95 | 27.06 |
| 10 | 1.62 | 13.38 | 0.81 | 9.76 | 16.29 | **35.92** | 15.96 | 27.32 |
| 11 | 2.29 | 13.66 | 0.72 | 10.60 | 5.64 | 21.21 | 7.68 | 14.95 |
| 12 | 2.02 | 13.51 | **1.00** | 10.23 | 10.17 | 31.64 | 11.80 | 23.13 |
| 13 | 2.20 | **14.10** | 0.97 | 10.43 | 3.74 | 16.37 | 5.83 | 12.64 |
| **14** | 2.82 | **14.68** | **1.13** | **12.16** | 7.71 | 29.21 | 11.20 | 22.01 |
| 15 | 2.34 | 12.48 | **1.32** | 10.04 | 3.02 | 14.39 | 4.54 | 10.55 |
| 16 | 2.11 | 13.48 | 0.77 | **11.26** | 7.10 | 28.13 | 11.21 | 21.57 |
| 17 | 1.98 | 13.06 | 0.90 | 9.60 | 5.42 | 22.17 | 7.69 | 16.35 |
| 18 | 2.16 | 12.34 | **1.21** | 10.29 | 10.84 | **33.09** | 13.73 | 24.77 |
| 19 | 2.42 | **14.86** | **1.08** | **11.82** | 4.05 | 18.03 | 6.33 | 13.90 |
| 20 | 2.11 | 12.31 | **1.12** | 10.02 | 8.25 | 29.06 | 10.77 | 21.59 |
| 21 | 2.29 | **14.39** | 0.95 | **11.50** | 3.86 | 17.17 | 4.85 | 12.77 |
| 22 | 1.80 | 11.36 | 0.50 | 8.79 | 7.83 | 28.13 | 11.44 | 21.09 |

单从测试结果来看，用 Freeze 或者 LoRA 微调模型最后 3 层的 QKV 矩阵效果还不错，之后打算从这个设置出发做进一步的实验。

