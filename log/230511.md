# 5 月 11 日实验日志

作者：hiyouga

数据集：koikake_yui_desc, koikake_yui_train

实验环境：A100 * 1

---

今天打算基于 5 月 10 日的微调模型做 RLHF 训练，探索该方法对回答质量是否有提升。在第一步中，使用中英文对比数据训练奖励模型，训练时依旧加上统一的 prompt，以对齐微调和 PPO 模型。

```bash
CUDA_VISIBLE_DEVICES=0 python ~/chatglm_tuning/src/train_rm.py \
    --do_train \
    --model_name_or_path ~/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a8ede826cf1b62bd3c78bdfb3625c7c5d2048fbd \
    --dataset comparison_gpt4_en,comparison_gpt4_zh \
    --dataset_dir ~/chatglm_tuning/data \
    --finetuning_type lora \
    --output_dir reward_model_230510_en_zh \
    --overwrite_cache \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 1e-5 \
    --weight_decay 1e-5 \
    --num_train_epochs 1.0 \
    --lora_target query_key_value \
    --source_prefix 现在你将扮演结衣，模拟学长和结衣的对话。结衣，你是御影浜学校的少女，喜欢学长、绘本、安静的地方、花和小动物。请根据角色性格特点，补全对话，下面是对话片段。 \
    --plot_loss \
    --fp16
```

在全部数据集上训练一轮以后，训练损失在 0.05 左右，为了防止过拟合没有训练更多轮次。

![1.jpg](media/230511_1.jpg)

接着使用训练好的微调模型和奖励模型进行强化学习训练。

```bash
CUDA_VISIBLE_DEVICES=0 python ~/chatglm_tuning/src/train_ppo.py \
    --do_train \
    --model_name_or_path ~/.cache/huggingface/hub/models--THUDM--chatglm-6b/snapshots/a8ede826cf1b62bd3c78bdfb3625c7c5d2048fbd \
    --dataset koikake_yui_desc,koikake_yui_train \
    --dataset_dir ../data \
    --finetuning_type lora \
    --checkpoint_dir  yui_230510_lora_qkv_100r \
    --reward_model reward_model_230510_en_zh \
    --output_dir yui_230511_lora_rlhf_50r \
    --overwrite_cache \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --max_source_length 256 \
    --max_target_length 128 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 1e-5 \
    --weight_decay 1e-5 \
    --num_train_epochs 50.0 \
    --lora_target query_key_value \
    --source_prefix 现在你将扮演结衣，模拟学长和结衣的对话。结衣，你是御影浜学校的少女，喜欢学长、绘本、安静的地方、花和小动物。请根据角色性格特点，补全对话，下面是对话片段。 \
    --plot_loss \
    --fp16
```


强化学习训练到 2000 步左右基本可以将 loss 优化到 1.0 以内，在 4000 步以后 loss 波动趋向于平稳。这里没有画 reward 曲线，将在之后实验记录中予以补充。

![2.jpg](media/230511_2.jpg)

采用训练结束后的 PPO 模型进行测试，起初发现模型的回答基本和微调之前的原始模型一致，猜测 PPO 训练过程中 KL 散度惩罚项过强，导致模型参数向原始模型参数过度偏移。于是我们加载了仅训练 1000 步的 PPO 模型测试效果，发现对话效果相比原始模型和仅使用 SFT 微调后的模型有所提升，具体测试结果如下。

![3.jpg](media/230511_3.jpg)

使用相同的对话在原始模型上进行测试，结果如下。相比 PPO 模型，原始模型会添加许多无关内容，形式也不像是人机之间的对话。

![4.jpg](media/230511_4.jpg)

再使用上述对话在仅使用 SFT 微调后的模型上进行测试，结果如下。相比 PPO 模型，SFT 模型倾向于给出很短的回复，且缺乏信息量。

![5.jpg](media/230511_5.jpg)

根据以上的结果，可以发现 PPO 模型在对话流畅度方面显著超过了 SFT 模型，并且相较原始模型，能一定程度地学习到人物的对话风格。在之后的实验中，打算从以下两方面加强模型训练：

1. 在 SFT 阶段加入更多数据集，让模型吸纳更多关于人物对话风格的知识。
2. 设计效果更好的 prompt 进行微调，或者尝试移除 prompt，观察其对实验结果的影响。
